---
layout: article
title: Впечатления практика
tags: Impressions
aside:
  toc: true
---
Автор: Алексей Гольдин, Teza.

Я приехал от нашей  trading компании не для того чтобы представить статью,
а для того чтобы понять состояние области искусственного интеллекта и
нейросетей, и при возможности найти новые идеи для
использования в нашей компании. Надежды, как обычно,
оправдались. Конференция -- это повод на несколько дней вынырнуть из
рутины и сконцентрироваться именно на обзоре нового.

Я люблю ICLR по нескольким причинам. Во первых, несмотря на огромное
количество участников, она заметно меньше ICML и NIPS. Во всех случаях
нельзя объять необъятное, но, почему-то, от мысли что удалось увидеть,
скажем, 1/10 а не 1/100 всех материалов становится теплее на душе. Во
вторых, в последние годы ICLR чаще других двух больших конференций
происходит за пределами Северной Америки, и поэтому состав участников
более интернациональный (многим трудно получить визу в США или
Канаду).

Что было примечательно на конференции с моей точки зрения, человека
который не двигает науку о нейросетях, а внедряет ее в жизнь? Это очень
субъективный взгляд, и то что интересно мне не обязательно интересно
широкой аудитории.

Не заостряя внимания на конкретных примерах того что мы собираемся
внедрять в своей фирме, я бы отметил несколько следующих работ.

Очень порадовала работа из Яндекса, которые долго и упорно
разрабатывают тему нейросетей для табулярных данных. Их работа про
[TabM](https://arxiv.org/abs/2410.24210)  внешне очень проста идейно, проверена на очень
большом количестве данных, дает очень неплохие результаты и довольно
проста в использовании.

Другая интересная работа от Uber расширяет область применимости
TabPFN. Это foundational модель для табулярных данных, которая не
требует тренировки для каждой конкретной задачи, но выдает результат
посмотрев на некоторые примеры. В этой работе: [(Mixture of In-Context Prompters for Tabular PFNs)](https://arxiv.org/abs/2405.16156)  для
каждого случая ищется небольшое количество релевантных данных
как примеры. Можно сказать, это RAG для табулярных данных.

Есть и много работ о временных рядах с непрекращающимися попытками
создать foundational model для временных рядов. Результаты, насколько
я понимаю, пока не очень, по крайней мере попытка использовать 
[TimeMOE](https://arxiv.org/abs/2409.16040) в zero shot варианте на интересных мне данных дала
душераздирающие результаты. Буду дожидаться варианта с многими переменными. 

[arXiv:2406.06811](https://arxiv.org/abs/2406.06811) показывает как тренировать модели которые легче
дообучать, [arXiv:2311.01434](https://arxiv.org/abs/2311.01434) показывает как заставить Mixup создавать
аугментации которые лежат недалеко от многообразия
данных. Интереснейшая и практичная работа: [Data Shapley in One
Training Run](https://arxiv.org/abs/2406.11011). Есть немало работ развивающих идеи
Kolmogorov-Arnold networks, но пока они скорее интересны чем
практичны.

Большая часть конференции была посвящена LLM, в которых я плохо
разбираюсь, но которые мне, как и многим, очень интересны. Не уверен,
насколько это прорывная работа, но мне понравилась идея в
[arXiv:2406.13629](https://arxiv.org/abs/2406.13629) --- как настроить модель, чтоб она лучше использовала
RAG.

Как бывшему астрофизику, мне, конечно, также были интересны статьи
посвященные применению нейросетей в науке, например, для ускорения
решения уравнений в частных производных. Интересно проверить,
действительно ли так хорошо работает метаоптимизация как она
описывается в статье [arXiv:2410.19746](https://arxiv.org/abs/2410.19746) . Очень интересен был также
[Workshop on Machine Learning Multiscale Processes](https://multiscale-ai.github.io/).

Очень заметно китайское присутствие. Как мне кажется, было немало
интересных работ, причем многие их авторы, увы, не очень хорошо
говорят по английски. В принципе, если нынешняя траектория сохранится,
возможно, английский им просто не будет нужен. Я бы посоветовал всем,
кто помоложе учить китайский в дополнение к обязательному
английскому. 大家好！В будущем будет чему поучиться.

В заключение хотелось бы выразить огромную благодарность Андрею
Устюжанину, который в далеком 2015 году убедил меня что нейросетки ---
это то, где надо копать. Это изменило мою жизнь в лучшую и более
интересную сторону.
